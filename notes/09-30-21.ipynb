{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d34abf2d",
   "metadata": {},
   "source": [
    "# Generative models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e774b620",
   "metadata": {},
   "source": [
    "## Intro and in the context of enhancer classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae14ab2",
   "metadata": {},
   "source": [
    "- Generative models will try and learn what composes one enhancer class vs another\n",
    "- Generative models generally fall into the un-supervised learning class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fdc36f",
   "metadata": {},
   "source": [
    "### What is the \"generative\" in generative model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099ed324",
   "metadata": {},
   "source": [
    "- Basically a probilility distribution that we can draw from (sample)\n",
    "\n",
    "$L$ = $10^5$ enhancers\n",
    "\n",
    "Given a sequence could call it enhancer or not enhancer. Generative models would assign probibility to a given enhancer. Model is generative because could sample from it.\n",
    "\n",
    "If $P(X)$ (the generative model) if the model was perfect sampling from $P(X)$ would be the same as sampling from all actual enhancers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a5071b",
   "metadata": {},
   "source": [
    "### What does it mean to \"fit\" a generative model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef65740",
   "metadata": {},
   "source": [
    "- We do not actually get to check $L$\n",
    "    - Have a sample of part of $L$ that represent some proportion of $L$\n",
    "    - The model tries to determine what region of space the samples sit within\n",
    "    \n",
    "#### An example\n",
    "\n",
    "- Trying to classify apples and oranges\n",
    "- Input data is images, labels is fruit ID\n",
    "\n",
    "$P(y|x)$ = \"is the color orange in the image\" is only required to classify (descrimitive model approach).\n",
    "\n",
    "But if you want to sample the model and get pictures of apples the model has to have a more complex understanding of what makes an apple an apple and an orange an orange."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9973434",
   "metadata": {},
   "source": [
    "## Generative models and sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e9b946",
   "metadata": {},
   "source": [
    "- We are given some sequence and we want to generate sequences that look like it this could be done by\n",
    "    - Prob of each letter and sample that discrete distribution\n",
    "    - Randomly arrange the letters\n",
    "    - 1st order Markov model\n",
    "\n",
    "### How can the above be formalized?\n",
    "\n",
    "- Set of variables $X_ij$ representing the $jth$ letter of sequence $i$\n",
    "- $Xijk$ where $k$ = {1, 2, 3, 4} where integers represent identities of nucleotides and $X_{ijk}=1$ if $X_{ij} = k$\n",
    "    - $k$ is asking the question \"is the identity of this nucleotide the one specified by the value of k making this a binary variable\n",
    "    - Also a form of \"one-hot\" encoding\n",
    "\n",
    "$P(X_{ijk} | \\sigma ) = \\sigma_k$: single base\n",
    "\n",
    "So the prob of all input data (the complete sequence) is the product of the probabilities of all nucleotides of that sequence.\n",
    "\n",
    "$P(X | \\sigma) = \\prod_{i}^{}\\prod_{j}^{}P(X_{ijk} = 1 | \\sigma )$\n",
    "\n",
    "$\\sigma$ is a parameter that would actually be trained. Fit $\\sigma$ to the data so the distribution we end up with looks as much like the training data as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608a3eb4",
   "metadata": {},
   "source": [
    "### Now we have a notation how to train?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53efd7a",
   "metadata": {},
   "source": [
    "- We want to find the right $\\sigma$s that will match our training data\n",
    "- We can do this using max-like approach\n",
    "    - Calculate probibility of all sequenecs in data set to zero and solve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c1760e",
   "metadata": {},
   "source": [
    "$L = \\prod_{i}^{}\\prod_{j}^{}\\prod_{k}^{}\\sigma_k$\n",
    "\n",
    "Take log of all terms\n",
    "\n",
    "$Log(L) = \\sum_{}^{i} \\sum_{}^{j} \\sum_{}^{k}X_{ijk}log(\\sigma_k)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa01acae",
   "metadata": {},
   "source": [
    "Have a constrained problem (0 to 1) but solving in a non-constrained way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3abaef",
   "metadata": {},
   "source": [
    "Could have defined $\\sigma_k$ as a function of another variable $a$ and then optimize $ak$?\n",
    "\n",
    "- Look into La Grange multipliers: Solving optimization problems when you have a specific kind of constraint.\n",
    "- To enforce the constraint have to reword it as a function that equals 0.\n",
    "- Then take original likihood function and add the la grange multiplier times the multiplier you derived"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80bd3d9",
   "metadata": {},
   "source": [
    "This model is not plausible in bio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc21f4e4",
   "metadata": {},
   "source": [
    "### Other approaches\n",
    "\n",
    "It would be nice to be able to look back $k$ positions, we need to add $4^k"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
